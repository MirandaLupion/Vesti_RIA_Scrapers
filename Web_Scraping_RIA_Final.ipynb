{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIA.ru Web Scraper: About"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on keyword and date parameters, scrapes article text from RIA.ru \n",
    "# Script takes a keyword in English or Russian, \n",
    "# two four-digit years, and two one- or two-digit months\n",
    "# Edit the parameters in the last code block\n",
    "# Scraper the data as a .txt file\n",
    "# When importing into Excel, select UTF 8 and * as delim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "\n",
    "# getLinks----------------------------------------------------------------\n",
    "# Function takes a keyword, two four-digit years, and two one- or two-digit months\n",
    "\n",
    "def getLinks(query, yearStart, monthStart, yearEnd, monthEnd):\n",
    "    \n",
    "    # Make the query ASCII, if it's not\n",
    "    # Add it to the search URL\n",
    "    \n",
    "    quoted_query = quote(query)\n",
    "    searchURL = (f\"https://ria.ru/search/?query={quoted_query}\")\n",
    "    \n",
    "    # Create an empty list to hold links\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    # Pull the HTML while anticipating HTTP error\n",
    "    \n",
    "    try:\n",
    "        html = urlopen(searchURL)\n",
    "    \n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "    \n",
    "    # If no HTTP error, then check for HTML error\n",
    "    \n",
    "    else:\n",
    "        if html is None:\n",
    "            print(\"URL is not found. Existing program.\")\n",
    "            exit()\n",
    "            \n",
    "        # If no HTML error, proceed with cleaning the HTML\n",
    "        \n",
    "        else:\n",
    "            print(\"URL found.\")\n",
    "            bsObj = BeautifulSoup(html)        \n",
    "       \n",
    "    \n",
    "    if bsObj.span is None:\n",
    "        print(\"Tag was not found. Exiting program.\") \n",
    "        exit()\n",
    "    else:\n",
    "        print(\"Tag found.\")\n",
    "\n",
    "    \n",
    "    # Find and print number of hits \n",
    "\n",
    "    totalHits = (bsObj.find(\"div\", {\"class\":\"rubric-count m-active\"}).get_text())\n",
    "    totalHits = int(re.sub(\"[^0-9]\", \"\", totalHits))     \n",
    "    print(\"There are \" + str(totalHits) + \" hits for that key word combination.\")\n",
    "    print(\"Begin collecting relevant article links.\")\n",
    "    \n",
    "    # Find the total number of cycles to go through \n",
    "\n",
    "    cycles = math.ceil(totalHits/20) \n",
    "    print(\"This will take \" + str(cycles) + \" cycles.\")\n",
    "        \n",
    "    # Use a while statement to cycle through pages\n",
    "    \n",
    "    counter = 20\n",
    "    while cycles > 0:\n",
    "    \n",
    "        print(str(cycles) + \" cycles left.\")\n",
    "        \n",
    "        # Collect page links based on span tag and data-url attribute.\n",
    "        # Append it to the link list.\n",
    "    \n",
    "        for link in bsObj.findAll(\"span\"):\n",
    "            if 'data-url' in link.attrs:\n",
    "                links.append(link.attrs['data-url'])\n",
    "                #time.sleep(2)\n",
    "        \n",
    "        # Update search link and collect HTML for it\n",
    "\n",
    "        searchURL = searchURL + \"&offset=\" + str(counter)\n",
    "        html = urlopen(searchURL)\n",
    "        bsObj = BeautifulSoup(html)\n",
    "    \n",
    "        counter = counter + 20\n",
    "        cycles = cycles - 1\n",
    "    \n",
    "    print(\"There are \" + str(len(links)) + \" relevant articles before filtering by date.\")\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\"Now we will filter by date...\")\n",
    "    \n",
    "    # Filter by date\n",
    "    \n",
    "    # Make year and month strings\n",
    "    \n",
    "    yearStart = str(yearStart)\n",
    "    monthStart = str(monthStart)\n",
    "    yearEnd = str(yearEnd)\n",
    "    monthEnd = str(monthEnd)\n",
    "    \n",
    "    # If neccessary, add a proceeding 0 to the month inputs\n",
    "    \n",
    "    if len(monthStart) == 1:\n",
    "        monthStart = \"0\" + monthStart\n",
    "    else:\n",
    "        monthStart = monthStart\n",
    "    if len(monthEnd) == 1:\n",
    "        monthEnd = \"0\" + monthEnd\n",
    "    else:\n",
    "        monthEnd = monthEnd\n",
    "    \n",
    "    # One month only\n",
    "    \n",
    "    if yearStart == yearEnd and monthStart == monthEnd: \n",
    "        \n",
    "        # Combine year and month to form a match statement \n",
    "        \n",
    "        match = yearStart + monthStart\n",
    "        \n",
    "        # Dates fall within links from index 15 to 21\n",
    "        \n",
    "        links = [link for link in links if link[15:21] == match]\n",
    "        \n",
    "        # Return relevant links \n",
    "        \n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        print(\"There are \" + str(len(links)) + \" relevant articles after filtering by date.\")\n",
    "        return links\n",
    "    \n",
    "    # Multiple months/years\n",
    "    \n",
    "    else:\n",
    "        linksFiltered = []\n",
    "        \n",
    "        startParameter = int(yearStart + monthStart)\n",
    "        endParameter = int(yearEnd + monthEnd)\n",
    "        \n",
    "        for link in links:\n",
    "            if int(link[15:21])>= startParameter and int(link[15:21]) <= endParameter:\n",
    "                linksFiltered.append(link)\n",
    "\n",
    "         # Return relevant links \n",
    "        \n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        print(\"There are \" + str(len(linksFiltered)) + \" relevant articles after filtering by date.\")\n",
    "        return linksFiltered\n",
    "\n",
    "\n",
    "# getMetaData----------------------------------------------------------------------\n",
    "def getMetaData(linkURL):\n",
    "    \n",
    "    html = urlopen(linkURL)\n",
    "    bsObj = BeautifulSoup(html.read());\n",
    "    \n",
    "    # Get the article title\n",
    "    \n",
    "    titleList = bsObj.findAll(\"\", {\"class\":\"article__title\"})\n",
    "\n",
    "    # Check to make sure it exists\n",
    "    \n",
    "    if len(titleList) == 0:\n",
    "        title_return = \"No title\"\n",
    "    \n",
    "    # Makes a list of all tags that fit this parameter\n",
    "    # Iterates through the list and then get text strips all tags from the text\n",
    "    \n",
    "    else:\n",
    "        for title in titleList:\n",
    "            title_return = title.get_text()            \n",
    "        \n",
    "    \n",
    "    # Get the article date\n",
    "    \n",
    "    dList = bsObj.findAll(\"\", {\"itemprop\": \"dateModified\"})\n",
    "    \n",
    "    # Check to make sure it exists\n",
    "    \n",
    "    if len(dList) == 0:\n",
    "        date_return = \"No date listed\"\n",
    "    \n",
    "    # Makes a list of all tags that fit this parameter\n",
    "    # Iterates through the list and then get text strips all tags from the text\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for d in dList:\n",
    "            date_return = d.get_text()[0:10]\n",
    "    \n",
    "    # Get article tags\n",
    "    \n",
    "    tList = bsObj.findAll(\"\", {\"class\":\"article__tags\"})\n",
    "\n",
    "    # Check to make sure it exists\n",
    "    \n",
    "    if len(tList) == 0:\n",
    "        tag_return = \"No tags listed\"\n",
    "        \n",
    "    # Makes a list of all tags that fit this parameter\n",
    "    # Iterates through the list and then get text strips all tags from the text\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for t in tList:\n",
    "            tag_return = t.get_text()\n",
    "    \n",
    "    # Return items\n",
    "    \n",
    "    return title_return, date_return, tag_return\n",
    "\n",
    "\n",
    "# getText------------------------------------------------------------------------\n",
    "\n",
    "def getText(linkURL):\n",
    "    text_return = \" \"\n",
    "    html = urlopen(linkURL)\n",
    "    bsObj = BeautifulSoup(html.read());\n",
    "    \n",
    "    # Get the aritcle text\n",
    "\n",
    "    textList = bsObj.findAll(\"\", {\"class\": \"article__text\"})\n",
    "    \n",
    "    \n",
    "    if len(textList) == 0:\n",
    "        text_return = \"Text under a different tag\"\n",
    "    else:\n",
    "        # Makes a list of all tags that fit this parameter\n",
    "        \n",
    "        for text in textList:\n",
    "            \n",
    "            # Iterates through the list and then get text strips all tags from the text\n",
    "            \n",
    "            text_return = text_return + (text.get_text())\n",
    "    \n",
    "    \n",
    "    text_return = re.sub(r'[\\n\\r]+', '', text_return)\n",
    "    \n",
    "    \n",
    "    return text_return\n",
    " \n",
    "# scrapeRIA----------------------------------------------------------------\n",
    "def scrapeRIA(query, yearStart, monthStart, yearEnd, monthEnd):\n",
    "    global title_return, date_return, tag_return\n",
    "    \n",
    "    # Use getLinks to retrieve all the links and save to list pullList \n",
    "    \n",
    "    pullList = getLinks(query, yearStart, monthStart, yearEnd, monthEnd)\n",
    "    \n",
    "    # Create empty lists to hold scraped content \n",
    "    \n",
    "    URLsLst = []\n",
    "    metadataLst = []\n",
    "    textLst = []\n",
    "    dateLst = []\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\"Scraping article text.\")\n",
    "   \n",
    "    for link in pullList:\n",
    "        print(link)\n",
    "        URLsLst.append(link)\n",
    "        print(\" \")\n",
    "        \n",
    "        metadata = getMetaData(link)\n",
    "        print(metadata)\n",
    "        metadataLst.append(metadata)\n",
    "        print(\" \")\n",
    "        \n",
    "        text2 = getText(link)\n",
    "        print(text2)\n",
    "        textLst.append(text2)\n",
    "        print(\" \")\n",
    "        \n",
    "        dateLst.append(link[15:19] + \"-\" + link[19:21] + \"-\" + link[21:23])\n",
    "    \n",
    "    # Create an empty data frame\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\"Building data frame...\")\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Add lists to data frame\n",
    "    \n",
    "    df[\"URLS\"] = URLsLst\n",
    "    df[\"metadata\"] = metadataLst\n",
    "    df[\"content\"] = textLst\n",
    "    df[\"date\"] = dateLst\n",
    "    \n",
    "    \n",
    "    print(df)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\"Exporting data...\")\n",
    "    \n",
    "    # Export data frame as a text file. \n",
    "    # When importing into Excel, select UTF 8 and * as delim.\n",
    "    \n",
    "    df.to_csv(\"RIA_data.txt\", sep='*', index=False)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Parameters and Run Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these \n",
    "\n",
    "query = \"sample query\"\n",
    "yearStart = 1\n",
    "monthStart = 2020\n",
    "yearEnd = 4\n",
    "monthEnd = 2020\n",
    "\n",
    "# Function takes a keyword, two four-digit years, and two one- or two-digit months\n",
    "\n",
    "scrapeRIA(query, yearStart, monthStart, yearEnd, monthEnd) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
